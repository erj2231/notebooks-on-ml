{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2032f23",
   "metadata": {},
   "source": [
    "functions - not oneliners\n",
    "math - not just functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97f935f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "data = pd.read_csv('''C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\notebooks-on-ml\\\\Coursera lil proj\\\\DATA\\\\a_steam_data_2021_2025.csv''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "954a8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['genres'] = data['genres'].fillna(data.groupby('categories')['genres'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan))\n",
    "data['genres'] = data['genres'].replace(['nan', 'None', ''], np.nan)\n",
    "data['categories'] = data['categories'].replace(['nan', 'None', ''], np.nan)\n",
    "data = data.dropna(subset=['genres', 'categories'])\n",
    "data['developer'] = data['developer'].fillna('Noname')\n",
    "data['publisher'] = data['publisher'].fillna('Noname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ad9a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['release_date'] = pd.to_datetime(data['release_date'], format='mixed', errors='coerce')\n",
    "data['release_date'] = data['release_date'].fillna(pd.to_datetime('Aug 1 ' + data['release_year'].astype(str), format='%b %d %Y', errors='coerce'))       \n",
    "data['genres'] = data['genres'].astype(str).str.split(';')\n",
    "data['categories'] = data['categories'].astype(str).str.split(';')\n",
    "data = data.sort_values(['release_date', 'developer']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a82ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_sequel'] = data['name'].apply(lambda x: 1 if any(word.upper() in ['2', '3', 'II', 'III', 'IV', 'V'] for word in str(x).split()) else 0).astype(int)\n",
    "data['developer&publisher'] = [1 if any(i in p for i in d) else 0 for d, p in zip(data['developer'], data['publisher'])]\n",
    "\n",
    "data['genre_count'] = data['genres'].apply(len)\n",
    "data['categories_count'] = data['categories'].apply(len)\n",
    "\n",
    "data['niche'] = data['genres'].str[0] + '_' + data['categories'].str[0]\n",
    "dev_gen_seen, dev_cat_seen = {}, {}\n",
    "dg_hist, dc_hist = [], []\n",
    "\n",
    "dev_gen_seen, dev_cat_seen = {}, {}\n",
    "pub_gen_seen, pub_cat_seen = {}, {}\n",
    "dg_hist, dc_hist, pg_hist, pc_hist = [], [], [], []\n",
    "\n",
    "for d, p, g, c in zip(data['developer'], data['publisher'], data['genres'], data['categories']):\n",
    "    d_key = tuple(d) if isinstance(d, list) else d\n",
    "    p_key = tuple(p) if isinstance(p, list) else p\n",
    "    \n",
    "    dg_hist.append(list(dev_gen_seen.get(d_key, [])))\n",
    "    dc_hist.append(list(dev_cat_seen.get(d_key, [])))\n",
    "    pg_hist.append(list(pub_gen_seen.get(p_key, [])))\n",
    "    pc_hist.append(list(pub_cat_seen.get(p_key, [])))\n",
    "    \n",
    "    dev_gen_seen.setdefault(d_key, set()).update(g)\n",
    "    dev_cat_seen.setdefault(d_key, set()).update(c)\n",
    "    pub_gen_seen.setdefault(p_key, set()).update(g)\n",
    "    pub_cat_seen.setdefault(p_key, set()).update(c)\n",
    "\n",
    "data['developer_genres'] = dg_hist\n",
    "data['developer_categories'] = dc_hist\n",
    "data['publisher_genres'] = pg_hist\n",
    "data['publisher_categories'] = pc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1a165f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_price'] = np.log1p(data['price'])\n",
    "data['log_recommendations'] = np.log1p(data['recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8032e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_hit'] = (data['log_recommendations'] > 800).astype(int)\n",
    "#data['is_mega_hit'] = data['log_recommendations'] > 1800\n",
    "data['is_discount'] = (data['log_price'] % 1 > 0.90).astype(int)\n",
    "data['release_month'] = data['release_date'].dt.month\n",
    "data['month_sin'] = np.sin(2 * np.pi * data['release_month'] / 12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data['release_month'] / 12)\n",
    "data['genre_count'] = data['genres'].apply(len)\n",
    "data['categories_count'] = data['categories'].apply(len)\n",
    "data['niche'] = data['genres'].str[0] + '_' + data['categories'].str[0]\n",
    "#data['price_per_genre'] = data['log_price'] / (data['genre_count'] + 1)\n",
    "\n",
    "dev_stats = {}\n",
    "pub_stats = {}\n",
    "\n",
    "res = {k: [] for k in [\n",
    "    'dev_hit_ratio', 'pub_hit_ratio', 'dev_avg_price', 'pub_avg_price',\n",
    "    'dev_std_price', 'pub_std_price', 'dev_avg_recs', 'pub_avg_recs',\n",
    "    'dev_mom', 'pub_mom', 'dev_gen_h', 'dev_cat_h', 'pub_gen_h', 'pub_cat_h'\n",
    "]}\n",
    "\n",
    "for d, p, h, pr, rec, g, c in zip(data['developer'], data['publisher'], data['is_hit'], \n",
    "                                  data['log_price'], data['log_recommendations'], data['genres'], data['categories']):\n",
    "    \n",
    "    for ent, stats, pref in [(d, dev_stats, 'dev'), (p, pub_stats, 'pub')]:\n",
    "        if ent not in stats:\n",
    "            stats[ent] = [0, 0, 0.0, 0.0, 0.0, 0.0, set(), set()]\n",
    "        \n",
    "        s = stats[ent]\n",
    "        n = s[1]\n",
    "\n",
    "        res[f'{pref}_hit_ratio'].append(s[0]/n if n>0 else 0)\n",
    "        res[f'{pref}_avg_price'].append(s[2]/n if n>0 else 0)\n",
    "        res[f'{pref}_avg_recs'].append(s[4]/n if n>0 else 0)\n",
    "        res[f'{pref}_mom'].append(rec - s[5] if n>0 else 0)\n",
    "        res[f'{pref}_gen_h'].append(list(s[6]))\n",
    "        res[f'{pref}_cat_h'].append(list(s[7]))\n",
    "        \n",
    "        if n > 1:\n",
    "            var = (s[3] / n) - (s[2] / n)**2\n",
    "            res[f'{pref}_std_price'].append(np.sqrt(max(0, var)))\n",
    "        else:\n",
    "            res[f'{pref}_std_price'].append(0)\n",
    "\n",
    "        s[0] += h\n",
    "        s[1] += 1\n",
    "        s[2] += pr\n",
    "        s[3] += pr**2\n",
    "        s[4] += rec\n",
    "        s[5] = rec\n",
    "        s[6].update(g)\n",
    "        s[7].update(c)\n",
    "\n",
    "data['developer_hit_ratio'] = res['dev_hit_ratio']\n",
    "data['publisher_hit_ratio'] = res['pub_hit_ratio']\n",
    "data['developer_price'] = res['dev_avg_price']\n",
    "data['publisher_price'] = res['pub_avg_price']\n",
    "data['developer_std'] = res['dev_std_price']\n",
    "data['publisher_std'] = res['pub_std_price']\n",
    "data['developer_recommendations'] = res['dev_avg_recs']\n",
    "data['publisher_recommendations'] = res['pub_avg_recs']\n",
    "data['developer_momentum'] = res['dev_mom']\n",
    "data['publisher_momentum'] = res['pub_mom']\n",
    "\n",
    "#data['unique_genres'] = [len(x) for x in res['dev_gen_h']]\n",
    "#data['unique_categories'] = [len(x) for x in res['dev_cat_h']]\n",
    "\n",
    "#data['developer_genre_overlap'] = [len(set(g) & set(h))/len(g) if len(g)>0 and h else 0 for g, h in zip(data['genres'], res['dev_gen_h'])]\n",
    "#data['developer_category_overlap'] = [len(set(c) & set(h))/len(c) if len(c)>0 and h else 0 for c, h in zip(data['categories'], res['dev_cat_h'])]\n",
    "#data['publisher_genre_overlap'] = [len(set(g) & set(h))/len(g) if len(g)>0 and h else 0 for g, h in zip(data['genres'], res['pub_gen_h'])]\n",
    "#data['publisher_category_overlap'] = [len(set(c) & set(h))/len(c) if len(c)>0 and h else 0 for c, h in zip(data['categories'], res['pub_cat_h'])]\n",
    "\n",
    "data['developer_game_number'] = data.groupby('developer').cumcount()\n",
    "data['developer_game_number'] = data.groupby('developer')['developer_game_number'].shift(1).fillna(0).astype(int)\n",
    "data['publisher_game_number'] = data.groupby('publisher').cumcount()\n",
    "data['publisher_game_number'] = data.groupby('publisher')['publisher_game_number'].shift(1).fillna(0).astype(int)\n",
    "\n",
    "data['publisher_niche_count'] = data.groupby(['publisher', 'niche']).cumcount()\n",
    "data['publisher_niche_count'] = data.groupby(['publisher', 'niche'])['publisher_niche_count'].shift(1).fillna(0).astype(int)\n",
    "data['developer_niche_count'] = data.groupby(['developer', 'niche']).cumcount()\n",
    "data['developer_niche_count'] = data.groupby(['developer', 'niche'])['developer_niche_count'].shift(1).fillna(0).astype(int)\n",
    "\n",
    "data['is_new_niche'] = ~data.duplicated(['developer', 'niche'])\n",
    "data['total_niches_count'] = data.groupby('developer')['is_new_niche'].cumsum()\n",
    "data['total_niches_count'] = data.groupby('developer')['total_niches_count'].shift(1).fillna(0).astype(int)\n",
    "data['is_specialist'] = (data['total_niches_count'] <= 2).astype(int)\n",
    "#data['is_veteran'] = (data['developer_niche_count'] >= 3).astype(int)\n",
    "\n",
    "#data['timeliness'] = data['recommendations'] / ((2026 - data['release_date'].astype(str).str.split('-').str[0].astype(int)) * 12 - (data['release_date'].astype(str).str.split('-').str[1].astype(int) - 1)).replace(0, 1)\n",
    "#data['quality'] = data['log_recommendations'] / (data['log_price'] + 0.001)\n",
    "#data['developer_quality_ratio'] = data.groupby('developer')['quality'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "#data['publisher_quality_ratio'] = data.groupby('publisher')['quality'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "#data['developer_niche_recommendations'] = data.groupby(['developer', 'niche'])['log_recommendations'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "#data['publisher_niche_recommendations'] = data.groupby(['publisher', 'niche'])['log_recommendations'].transform(lambda x: x.shift(1).expanding().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98bf43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_developer_price'] = np.log1p(data['developer_price'])\n",
    "#data['log_publisher_price'] = np.log1p(data['publisher_price'])\n",
    "data['log_developer_recommendations'] = np.log1p(data['developer_recommendations'])\n",
    "data['log_publisher_recommendations'] = np.log1p(data['publisher_recommendations'])\n",
    "#data['log_quality'] = np.log1p(data['quality'])\n",
    "#data['log_developer_quality_ratio'] = np.log1p(data['developer_quality_ratio'])\n",
    "#data['log_publisher_quality_ratio'] = np.log1p(data['publisher_quality_ratio'])\n",
    "data['log_developer_momentum'] = np.sign(data['developer_momentum']) * np.log1p(np.abs(data['developer_momentum']))\n",
    "#data['log_developer_niche_recommendations'] = np.log1p(data['developer_niche_recommendations'])\n",
    "data['log_publisher_momentum'] = np.sign(data['publisher_momentum']) * np.log1p(np.abs(data['publisher_momentum']))\n",
    "#data['log_publisher_niche_recommendations'] = np.log1p(data['publisher_niche_recommendations'])\n",
    "data['log_total_niches_count'] = np.log1p(data['total_niches_count'])\n",
    "data['log_developer_game_number'] = np.log1p(data['developer_game_number'])\n",
    "data['log_publisher_game_number'] = np.log1p(data['publisher_game_number'])\n",
    "data['log_developer_std'] = np.log1p(data['developer_std']) \n",
    "data['log_publisher_std'] = np.log1p(data['publisher_std'])\n",
    "\n",
    "data['developer_game_number'] = data.groupby('developer').cumcount() + 1\n",
    "data['publisher_game_number'] = data.groupby('publisher').cumcount() + 1\n",
    "data['log_developer_game_number'] = np.log1p(data['developer_game_number'])\n",
    "data['log_publisher_game_number'] = np.log1p(data['publisher_game_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ce361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "----------------\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#inf_counts = np.isinf(data.select_dtypes(include=[np.number])).sum()\n",
    "#print(inf_counts[inf_counts > 0])\n",
    "#print('----------------')\n",
    "#nan_counts = data.isnull().sum()\n",
    "#print(nan_counts[nan_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0d3e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_cols = [\n",
    "    'log_price', 'log_developer_price',\n",
    "    'log_developer_recommendations', 'log_publisher_recommendations', \n",
    "#    'log_quality', 'log_developer_quality_ratio', 'log_publisher_quality_ratio', \n",
    "#    'log_publisher_price', 'log_recommendations',\n",
    "    'log_developer_momentum', 'log_publisher_momentum',\n",
    "#    'log_developer_niche_recommendations', 'log_publisher_niche_recommendations',\n",
    "    'developer_niche_count', 'publisher_niche_count',\n",
    "#    'log_developer_count', 'log_publisher_count',\n",
    "    'log_total_niches_count',\n",
    "    'log_developer_std', 'log_publisher_std',\n",
    "]\n",
    "minmax_cols = [\n",
    "#    'timeliness', \n",
    "    'genre_count', 'categories_count',  \n",
    "#    'unique_genres', 'unique_categories',\n",
    "#    'developer_category_overlap', 'publisher_category_overlap',\n",
    "#    'developer_genre_overlap', 'publisher_genre_overlap',\n",
    "    'month_sin', 'month_cos'\n",
    "]\n",
    "robust = RobustScaler()\n",
    "data[robust_cols] = robust.fit_transform(data[robust_cols])\n",
    "mms = MinMaxScaler()\n",
    "data[minmax_cols] = mms.fit_transform(data[minmax_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04c3c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "genre_sparse = mlb.fit_transform(data['genres'])\n",
    "genre_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    genre_sparse, \n",
    "    columns=mlb.classes_, \n",
    "    index=data.index\n",
    ")\n",
    "category_sparse = mlb.fit_transform(data['categories'])\n",
    "category_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    category_sparse, \n",
    "    columns=mlb.classes_, \n",
    "    index=data.index\n",
    ")\n",
    "mobile_cols = [c for c in category_df.columns if 'Remote Play on Phone' in c or 'Remote Play on Tablet' in c]\n",
    "data['mobile'] = category_df[mobile_cols].any(axis=1).astype('int8')\n",
    "data = pd.concat([data.drop(columns=['genres', 'categories']), genre_df, category_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdce109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\n",
    "    'publisher', 'developer', 'price', 'name', 'niche', 'appid', 'release_date',\n",
    "    'publisher_genres', 'publisher_categories', 'developer_genres', 'developer_categories',\n",
    "    'publisher_price', 'developer_price', 'developer_recommendations', \n",
    "    'publisher_recommendations', 'total_niches_count', 'developer_game_number', \n",
    "    'publisher_game_number', 'release_year', 'release_month', 'recommendations', \n",
    "    'Remote Play on Phone', 'Remote Play on Tablet', 'Online PvP', 'Family Sharing',\n",
    "    'Online Co-op', 'Shared/Split Screen', 'publisher_std', 'developer_std', 'log_recommendations'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3381e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('''C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\notebooks-on-ml\\\\Coursera lil proj\\\\DATA\\\\transition_file.csv''', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48090b77",
   "metadata": {},
   "source": [
    "corr_matrix = data.select_dtypes(include=[np.number]).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(f\"Redundant columns to drop: {to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc75e26",
   "metadata": {},
   "source": [
    "pairs = upper.unstack().dropna()\n",
    "sorted_pairs = pairs.sort_values(ascending=False)\n",
    "print(\"Top 10 most redundant pairs:\")\n",
    "print(sorted_pairs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf4dc4",
   "metadata": {},
   "source": [
    "sns.scatterplot(x=data['publisher_std'], y=data['developer_std'])\n",
    "plt.title(\"Visualizing Redundancy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6e47c",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "subset = data.iloc[:, :30] \n",
    "sns.heatmap(subset.corr(), annot=False, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93d6c7",
   "metadata": {},
   "source": [
    "plot_data = data.explode('genres').explode('categories')\n",
    "sns.barplot(data=plot_data, x='release_year', y='price')\n",
    "plt.show()\n",
    "sns.barplot(data=plot_data, x='release_year', y='recommendations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d8e26",
   "metadata": {},
   "source": [
    "timeline_data = plot_data.groupby(['release_year', 'genres']).size().reset_index(name='count')\n",
    "timeline_data_2 = plot_data.groupby(['release_year', 'categories']).size().reset_index(name='count')\n",
    "sns.lineplot(data=timeline_data, x='release_year', y='count', hue='genres')\n",
    "plt.show()\n",
    "sns.lineplot(data=timeline_data_2, x='release_year', y='count', hue='categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46c317",
   "metadata": {},
   "source": [
    "sns.barplot(data=plot_data, x='price', y='genres')\n",
    "plt.show()\n",
    "sns.barplot(data=plot_data, x='price', y='categories')\n",
    "plt.show()\n",
    "sns.barplot(data=plot_data, x='recommendations', y='genres')\n",
    "plt.show()\n",
    "sns.barplot(data=plot_data, x='recommendations', y='categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36a51d",
   "metadata": {},
   "source": [
    "sns.histplot(data['price'], kde=True)\n",
    "plt.show()\n",
    "sns.histplot(data['recommendations'], kde=True)\n",
    "plt.show()\n",
    "sns.histplot(data['log_price'], kde=True)\n",
    "plt.show()\n",
    "sns.histplot(data['log_recommendations'], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e42b079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('''C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\notebooks-on-ml\\\\Coursera lil proj\\\\DATA\\\\transition_file.csv''')\n",
    "X = data.drop('is_hit', axis=1)\n",
    "y = data['is_hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e64604b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m multi_model = MultiOutputClassifier(stack)\n\u001b[32m     24\u001b[39m y = data[[\u001b[33m'\u001b[39m\u001b[33mis_hit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mis_sequel\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmulti_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\multioutput.py:543\u001b[39m, in \u001b[36mMultiOutputClassifier.fit\u001b[39m\u001b[34m(self, X, Y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, **fit_params):\n\u001b[32m    518\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[32m    519\u001b[39m \n\u001b[32m    520\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m \u001b[33;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m     \u001b[38;5;28mself\u001b[39m.classes_ = [estimator.classes_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.estimators_]\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\multioutput.py:274\u001b[39m, in \u001b[36m_MultiOutputEstimator.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    272\u001b[39m         routed_params.estimator.fit[\u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m] = sample_weight\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mn_features_in_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_features_in_ = \u001b[38;5;28mself\u001b[39m.estimators_[\u001b[32m0\u001b[39m].n_features_in_\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\utils\\parallel.py:184\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m                 this_warning_filter_dict[special_key] = this_value.pattern\n\u001b[32m    182\u001b[39m         warnings.filterwarnings(**this_warning_filter_dict, append=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\multioutput.py:63\u001b[39m, in \u001b[36m_fit_estimator\u001b[39m\u001b[34m(estimator, X, y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m     61\u001b[39m     estimator.fit(X, y, sample_weight=sample_weight, **fit_params)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:706\u001b[39m, in \u001b[36mStackingClassifier.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;28mself\u001b[39m.classes_ = \u001b[38;5;28mself\u001b[39m._label_encoder.classes_\n\u001b[32m    704\u001b[39m     y_encoded = \u001b[38;5;28mself\u001b[39m._label_encoder.transform(y)\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:277\u001b[39m, in \u001b[36m_BaseStacking.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28mself\u001b[39m.stack_method_ = [\n\u001b[32m    271\u001b[39m     meth\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.stack_method_, all_estimators)\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m est != \u001b[33m\"\u001b[39m\u001b[33mdrop\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    274\u001b[39m ]\n\u001b[32m    276\u001b[39m X_meta = \u001b[38;5;28mself\u001b[39m._concatenate_predictions(X, predictions)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m \u001b[43m_fit_single_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinal_estimator_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:45\u001b[39m, in \u001b[36m_fit_single_estimator\u001b[39m\u001b[34m(estimator, X, y, fit_params, message_clsname, message)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\system\\python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1243\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1240\u001b[39m     max_squared_sum = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1247\u001b[39m     )\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.warm_start:\n\u001b[32m   1250\u001b[39m     warm_start_coef = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcoef_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "base_models = [\n",
    "    ('xgb', XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=5, use_label_encoder=False, base_score=0.5, eval_metric='logloss')),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_leaf=5))\n",
    "]\n",
    "up_model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "stack = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=up_model,\n",
    "    cv=skf,\n",
    "    stack_method='predict_proba',\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "multi_model = MultiOutputClassifier(stack)\n",
    "y = data[['is_hit', 'is_sequel']]\n",
    "multi_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1afbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "=== Perfomance for: is_hit ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63157\n",
      "           1       1.00      1.00      1.00      2354\n",
      "\n",
      "    accuracy                           1.00     65511\n",
      "   macro avg       1.00      1.00      1.00     65511\n",
      "weighted avg       1.00      1.00      1.00     65511\n",
      "\n",
      "f\n",
      "=== Perfomance for: is_sequel ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     63793\n",
      "           1       1.00      1.00      1.00      1718\n",
      "\n",
      "    accuracy                           1.00     65511\n",
      "   macro avg       1.00      1.00      1.00     65511\n",
      "weighted avg       1.00      1.00      1.00     65511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = multi_model.predict(X)\n",
    "for index, name in enumerate(['is_hit', 'is_sequel']):\n",
    "    print(f'f\\n=== Perfomance for: {name} ===')\n",
    "    print(classification_report(y.iloc[:, index], preds[:, index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89caec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "is_hit",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8efc3a80-82b1-4711-a501-30aabb450547",
       "rows": [
        [
         "is_hit",
         "1.0"
        ],
        [
         "log_recommendations",
         "0.6675341758181007"
        ],
        [
         "publisher_hit_ratio",
         "0.35407751645412516"
        ],
        [
         "Steam Trading Cards",
         "0.3033576974810729"
        ],
        [
         "log_publisher_recommendations",
         "0.29580824497116714"
        ],
        [
         "developer_hit_ratio",
         "0.23857696644208243"
        ],
        [
         "log_publisher_momentum",
         "0.22750169201365084"
        ],
        [
         "log_price",
         "0.21390939568059217"
        ],
        [
         "categories_count",
         "0.1960699925139311"
        ],
        [
         "Steam Cloud",
         "0.18933803086452264"
        ],
        [
         "log_developer_recommendations",
         "0.18349907019855935"
        ],
        [
         "log_publisher_std",
         "0.158164291356095"
        ],
        [
         "log_developer_momentum",
         "0.14939008712137336"
        ],
        [
         "Steam Achievements",
         "0.13246732843734307"
        ],
        [
         "Full controller support",
         "0.12211255635799205"
        ],
        [
         "HDR available",
         "0.11126887070814499"
        ],
        [
         "Steam Workshop",
         "0.10940877943850438"
        ],
        [
         "Adjustable Text Size",
         "0.08773695278187221"
        ],
        [
         "Co-op",
         "0.0860777691771282"
        ],
        [
         "Surround Sound",
         "0.08517865950556251"
        ],
        [
         "Adjustable Difficulty",
         "0.08129104448056826"
        ],
        [
         "Camera Comfort",
         "0.07212699487698104"
        ],
        [
         "Multi-player",
         "0.0678862814443768"
        ],
        [
         "Stereo Sound",
         "0.0674928148351618"
        ],
        [
         "Color Alternatives",
         "0.06715570837930387"
        ],
        [
         "Subtitle Options",
         "0.060632485425612954"
        ],
        [
         "Cross-Platform Multiplayer",
         "0.06027218684290762"
        ],
        [
         "RPG",
         "0.05809853742645772"
        ],
        [
         "Chat Speech-to-text",
         "0.05799172070947178"
        ],
        [
         "Custom Volume Controls",
         "0.0550094575519504"
        ],
        [
         "publisher_momentum",
         "0.053852866752294684"
        ],
        [
         "is_discount",
         "0.05291115939504393"
        ],
        [
         "is_sequel",
         "0.0524854445318225"
        ],
        [
         "Simulation",
         "0.04985073110165012"
        ],
        [
         "Save Anytime",
         "0.04119416200759798"
        ],
        [
         "Captions available",
         "0.04001915046630559"
        ],
        [
         "log_publisher_game_number",
         "0.038968975437044376"
        ],
        [
         "Remote Play on TV",
         "0.03602923488545724"
        ],
        [
         "Partial Controller Support",
         "0.0353596984344354"
        ],
        [
         "Shared/Split Screen Co-op",
         "0.03375042274511006"
        ],
        [
         "Playable without Timed Input",
         "0.03351374923498022"
        ],
        [
         "Chat Text-to-speech",
         "0.033211396225427424"
        ],
        [
         "Includes level editor",
         "0.032193614909380386"
        ],
        [
         "log_developer_price",
         "0.032159499352605925"
        ],
        [
         "PvP",
         "0.03113599508698845"
        ],
        [
         "mobile",
         "0.02775171377385973"
        ],
        [
         "Adventure",
         "0.02652746121852123"
        ],
        [
         "developer_momentum",
         "0.02651542068778377"
        ],
        [
         "In-App Purchases",
         "0.026492977019828588"
        ],
        [
         "log_developer_std",
         "0.025420063031499983"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 99
       }
      },
      "text/plain": [
       "is_hit                           1.000000\n",
       "log_recommendations              0.667534\n",
       "publisher_hit_ratio              0.354078\n",
       "Steam Trading Cards              0.303358\n",
       "log_publisher_recommendations    0.295808\n",
       "                                   ...   \n",
       "developer_niche_count           -0.031098\n",
       "log_developer_game_number       -0.031859\n",
       "Indie                           -0.046192\n",
       "Free To Play                    -0.065472\n",
       "Casual                          -0.065741\n",
       "Name: is_hit, Length: 99, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = data.corr()['is_hit'].sort_values(ascending=False)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
